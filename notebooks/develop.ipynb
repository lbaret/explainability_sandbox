{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from typing import Dict, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from src.dataset_handler.classic_dataset import ClassicDataset\n",
    "from src.models.lightning_wrapper import LightningWrapper\n",
    "from src.models.multi_linear_layers import MultiLinearLayers\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchmetrics.classification import BinaryAccuracy\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoints = 'lightning_logs/version_23/checkpoints/epoch=49-step=5200.ckpt' # Keep it None if you want to train the model\n",
    "checkpoints = None\n",
    "\n",
    "df = pd.read_csv('../data/heart.csv')\n",
    "df.sample(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer :\n",
    "- Age : Standardiser entre 0 et 1\n",
    "- Sex : en binaire -> Homme = 0, Femme = 1\n",
    "- ChestPainType : en variable catégorique\n",
    "- RestingBP : Standardiser entre 0 et 1\n",
    "- Cholesterol : Standardiser entre 0 et 1\n",
    "- FastingBS : Ne rien faire (booléen)\n",
    "- RestingECG : en variable catégorique\n",
    "- MaxHR : Standardiser entre 0 et 1\n",
    "- ExerciseAngina : en binaire -> N = 0, Y = 1\n",
    "- Oldpeak : Standardiser entre 0 et 1\n",
    "- ST_Slope : en variable catégorique\n",
    "- HeartDisease : Ne rien faire (booléen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_sex(sex: str) -> int:\n",
    "    if sex.lower() == 'f':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def binarize_exercise_angina(exercise_angina: str) -> int:\n",
    "    if exercise_angina.lower() == 'y':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers = {\n",
    "    'Age': MinMaxScaler(),\n",
    "    'RestingBP': MinMaxScaler(),\n",
    "    'Cholesterol': MinMaxScaler(),\n",
    "    'MaxHR': MinMaxScaler(),\n",
    "    'Oldpeak': MinMaxScaler(),\n",
    "    'ChestPainType': OneHotEncoder(),\n",
    "    'RestingECG': OneHotEncoder(),\n",
    "    'ST_Slope': OneHotEncoder(),\n",
    "    'Sex': binarize_sex,\n",
    "    'ExerciseAngina': binarize_exercise_angina\n",
    "}\n",
    "\n",
    "single_values_columns = []\n",
    "multiple_values_columns = []\n",
    "for column_name, preprocessor in transformers.items():\n",
    "    if isinstance(preprocessor, MinMaxScaler):\n",
    "        df[column_name] = preprocessor.fit_transform(df[column_name].to_numpy().reshape(-1, 1))\n",
    "        single_values_columns.append(column_name)\n",
    "    elif isinstance(preprocessor, OneHotEncoder):\n",
    "        df[column_name] = preprocessor.fit_transform(df[column_name].to_numpy().reshape(-1, 1)).toarray().tolist()\n",
    "        multiple_values_columns.append(column_name)\n",
    "    else:\n",
    "        df[column_name] = df[column_name].apply(lambda val: preprocessor(val))\n",
    "        single_values_columns.append(column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['HeartDisease'].to_numpy()\n",
    "\n",
    "inputs_rows = []\n",
    "columns_order = []\n",
    "for _, row in df.drop(columns='HeartDisease').iterrows():\n",
    "    arr_row = [row[coln] for coln in single_values_columns]\n",
    "    [arr_row.extend(row[coln]) for coln in multiple_values_columns]\n",
    "\n",
    "    columns_order = single_values_columns + multiple_values_columns\n",
    "\n",
    "    inputs_rows.append(arr_row)\n",
    "\n",
    "labels = torch.tensor(labels, dtype=torch.float32).reshape(-1, 1)\n",
    "inputs = torch.tensor(inputs_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ClassicDataset(inputs, labels)\n",
    "\n",
    "total_size = len(dataset)\n",
    "train_ratio = 0.9\n",
    "train_size = int(train_ratio * total_size)\n",
    "test_size = total_size - train_size\n",
    "\n",
    "train_set, test_set = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=8, shuffle=True, num_workers=8)\n",
    "test_loader = DataLoader(test_set, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = MultiLinearLayers(inputs.shape[1], 1)\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = LightningWrapper(network, loss_function, metrics={'accuracy': BinaryAccuracy().to('cuda')})\n",
    "\n",
    "if checkpoints is None:\n",
    "    trainer = pl.Trainer(accelerator='gpu', max_epochs=10)\n",
    "\n",
    "    trainer.fit(model, train_loader)\n",
    "else:\n",
    "    model.load_from_checkpoint(checkpoint_path=checkpoints, checkpoint_callback=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bel entrainement maintenant, essayons de comprendre les distributions des neurones de chacune des couches, afin de savoir si c'est uniformément distribués où chaque neurone présente sa spécificité."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récupérons le test set pour pouvoir commencer à travailler dessus. Un modèle simple + un cas binaire pour généraliser ensuite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submodel = deepcopy(model.wrapped_model)\n",
    "\n",
    "# Autant tout faire passer d'un coup\n",
    "X_test = []\n",
    "y_test = []\n",
    "for x, y in test_set:\n",
    "    X_test.append(x.unsqueeze(0))\n",
    "    y_test.append(y)\n",
    "\n",
    "X_test = torch.cat(X_test)\n",
    "y_test = torch.cat(y_test)\n",
    "\n",
    "# On peut chercher les logits du modèle\n",
    "outputs = submodel(X_test)\n",
    "\n",
    "BinaryAccuracy()(outputs, y_test.unsqueeze(1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super, maintenant entammons une analyse poussée de nos couches en sorties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_neurons_collector = {}\n",
    "def forward_hook(module: nn.Module, inputs: torch.Tensor, outputs: torch.Tensor, name: str, out_neurons_collector: Dict[str, List[torch.Tensor]]) -> None:\n",
    "    if name in out_neurons_collector.keys():\n",
    "        out_neurons_collector[name].append(outputs.detach().cpu())\n",
    "    else:\n",
    "        out_neurons_collector[name] = [outputs.detach().cpu()]\n",
    "\n",
    "hooks = []\n",
    "for name, module in submodel.named_modules():\n",
    "    if name != '':\n",
    "        hooks.append(module.register_forward_hook(partial(forward_hook, name=name, out_neurons_collector=out_neurons_collector)))\n",
    "\n",
    "outputs = submodel(X_test)\n",
    "\n",
    "for h in hooks:\n",
    "    h.remove()\n",
    "\n",
    "for layer_name, neurons in out_neurons_collector.items():\n",
    "    out_neurons_collector[layer_name] = deepcopy(torch.cat(out_neurons_collector[layer_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_neurons_collector['linear2'].mean(dim=0), out_neurons_collector['linear2'].std(dim=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avons nous à faire à des distributions normales ? Un test statistique pourra nous le dire !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_samples = {}\n",
    "shapiro_pvalues = {}\n",
    "\n",
    "for layer_name, layer_tensor in out_neurons_collector.items():\n",
    "    normal_samples[layer_name] = []\n",
    "    shapiro_pvalues[layer_name] = []\n",
    "\n",
    "    for i in range(layer_tensor.shape[1]):\n",
    "        samples = layer_tensor[:, i]\n",
    "        shapiro_test = stats.shapiro(samples)\n",
    "\n",
    "        normal_samples[layer_name].append(True if shapiro_test.pvalue > 0.05 else False)\n",
    "        shapiro_pvalues[layer_name].append(shapiro_test.pvalue)\n",
    "\n",
    "    normal_samples[layer_name] = torch.BoolTensor(normal_samples[layer_name])\n",
    "    shapiro_pvalues[layer_name] = torch.Tensor(shapiro_pvalues[layer_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "sorted_indices = shapiro_pvalues['linear1'].sort()[1][:10]\n",
    "df_distplot = pd.DataFrame()\n",
    "for c, ind in enumerate(sorted_indices):\n",
    "    sns.distplot(out_neurons_collector['linear1'][:, ind.item()], ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Souvent bimodal, c'est intéressant !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "sorted_indices = shapiro_pvalues['linear1'].sort(descending=True)[1][:10]\n",
    "df_distplot = pd.DataFrame()\n",
    "for c, ind in enumerate(sorted_indices):\n",
    "    sns.distplot(out_neurons_collector['linear1'][:, ind.item()], ax=ax, label=ind)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ont-ils simplement un effet régularisateur ?\n",
    "\n",
    "Dans deux cas, où se situent les valeurs pour la classe 0 et la classe 1 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "sorted_indices = shapiro_pvalues['linear1'].sort()[1][:10]\n",
    "df_distplot = pd.DataFrame()\n",
    "for c, ind in enumerate(sorted_indices):\n",
    "    sns.distplot(out_neurons_collector['linear1'][:, ind.item()], ax=ax, label=ind)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices = shapiro_pvalues['linear1'].sort()[1][:10]\n",
    "\n",
    "df_linear_1_unnormal = pd.DataFrame(data=out_neurons_collector['linear1'][:, sorted_indices])\n",
    "df_linear_1_unnormal['label'] = y_test\n",
    "\n",
    "sns.displot(data=df_linear_1_unnormal.melt(id_vars=['label']), x='value', hue='label', col='variable', col_wrap=3, alpha=0.5, kde=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices = shapiro_pvalues['linear1'].sort(descending=True)[1][:10]\n",
    "\n",
    "df_linear_1_normal = pd.DataFrame(data=out_neurons_collector['linear1'][:, sorted_indices])\n",
    "df_linear_1_normal['label'] = y_test\n",
    "\n",
    "sns.displot(data=df_linear_1_normal.melt(id_vars=['label']), x='value', hue='label', col='variable', col_wrap=3, alpha=0.5, kde=True);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Admettons que nous avons des neurones que l'on considère comme dissociatifs, pouvons nous aisément repérer les poids ou l'ensemble des poids qui vont maintenir ou créer une dissociation par la suite ? \\\n",
    "Si les poids n'accentuent, ne conservent ou ne créent pas de dissociation alors ça va être difficile de repérer les neurones importants. \\\n",
    "Une de mes hypothèses est que les neurones dont les distributions sont confondues ne sont présents qu'à des fins de régulation et ne peuvent être considérés comme importants car non dissociés.\n",
    "\n",
    "La question que nous pouvons nous poser maintenant est : quels sont les neurones qui présentent la plus grosse dissociation ? Que ce soit fortement positif ou négatif. Si présence de ReLU alors la négation est considérée comme nul et alors le signal qui est émit ne vient que d'une classe en soi.\n",
    "\n",
    "Dans la méthode, nous ne pouvons négliger le théorème central limite, bien que nous ayons l'équivalent de 2 variables aléatoires par neurone (l'effet peut être plus remarquable avec plus de classes par exemple). L'union des deux distributions (de la classe 0 et de la classe 1) peut donner une distribution normale. Donc, il serait intéressant de mesurer 2 valeurs :\n",
    "1. La divergence de *Kullback-Leibler* (sûrement le plus intéressant)\n",
    "2. La moyenne et l'écart-type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.random.normal(loc=5.0, scale=1.5, size=(10000,))\n",
    "X2 = np.random.normal(loc=5.0, scale=1.5, size=(1000,))\n",
    "X3 = np.random.normal(loc=3.0, scale=1.0, size=(50,))\n",
    "X4 = np.random.normal(loc=5.0, scale=2.0, size=(10000,))\n",
    "\n",
    "stats.mannwhitneyu(X1, X2), stats.mannwhitneyu(X1, X3), stats.mannwhitneyu(X1, X4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_positive_1 = df_linear_1_normal[df_linear_1_normal['label'] == 1.][1].to_numpy()\n",
    "X_negative_1 = df_linear_1_normal[df_linear_1_normal['label'] == 0.][1].to_numpy()\n",
    "\n",
    "X_positive_9 = df_linear_1_normal[df_linear_1_normal['label'] == 1.][9].to_numpy()\n",
    "X_negative_9 = df_linear_1_normal[df_linear_1_normal['label'] == 0.][9].to_numpy()\n",
    "\n",
    "stats.mannwhitneyu(X_positive_1, X_negative_1), stats.mannwhitneyu(X_positive_9, X_negative_9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisons ce test statistique pour déterminer l'importance d'un neurone. Procédons par étapes :\n",
    "1. Récupérons le dictionnaire des outputs d'une couche\n",
    "2. Séparons les valeurs par rapport à leur classe\n",
    "3. Effectuons les tests statistiques suivants :\n",
    "   1. Si distributions normales ou proches : t-test (ou Student)\n",
    "   2. Sinon : test de Mann-Whitney U\n",
    "      1. Si semblables : cherchons voir si la moyenne/variance est similaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_is_important(samples: torch.Tensor, positive_indices: torch.Tensor, negative_indices: torch.Tensor) -> bool:\n",
    "    is_important = False\n",
    "    X_positive = samples[positive_indices]\n",
    "    X_negative = samples[negative_indices]\n",
    "\n",
    "    is_X1_normal = stats.shapiro(X_positive).pvalue > 0.05\n",
    "    is_X2_normal = stats.shapiro(X_negative).pvalue > 0.05\n",
    "\n",
    "    if is_X1_normal and is_X2_normal:\n",
    "        is_important = stats.ttest_ind(X_positive, X_negative, equal_var=False).pvalue < 0.05\n",
    "    else:\n",
    "        is_important = stats.mannwhitneyu(X_positive, X_negative).pvalue < 0.05\n",
    "    \n",
    "    return is_important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_indices = torch.where(y_test == 0)[0]\n",
    "positive_indices = torch.where(y_test == 1)[0]\n",
    "\n",
    "layers_important_neurons = {}\n",
    "layers_non_important_neurons = {}\n",
    "\n",
    "for layer_name, layer_outputs in out_neurons_collector.items():\n",
    "    if layer_name == 'fc':\n",
    "        continue\n",
    "    linear1 = out_neurons_collector['linear1']\n",
    "\n",
    "    important_neurons = torch.zeros(size=(layer_outputs.shape[1],), dtype=torch.bool)\n",
    "\n",
    "    for i in range(layer_outputs.shape[1]):\n",
    "        samples = layer_outputs[:, i]\n",
    "        important_neurons[i] = torch.BoolTensor([neuron_is_important(samples, positive_indices, negative_indices)])[0]\n",
    "\n",
    "    layers_important_neurons[layer_name] = torch.where(important_neurons == True)[0]\n",
    "    layers_non_important_neurons[layer_name] = torch.where(important_neurons == False)[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour tester notre méthode, prenons aléatoirement et en répétant 10 fois, des neurones importants, effacons les (à 0) puis évaluons la perte en performance (faire pareil avec les neurones pas importants)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_important_neurons"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour masquer les neurones il faut masquer tous les poids et biais arrivant à ce neurone. Ce faisant nous empêchons tout signal d'arriver jusqu'à ce neurone et donc il ne sera pas utile pour la suite des traitements.\n",
    "\n",
    "Cependant ce qu'il faut prendre en compte, c'est que cette méthode statistique ne permet pas de prendre en compte toutes les interactions entre les neurones des couches successives. Donc éteindre le signal provenant d'un neurone peut avoir une conséquence dans la couche suivante. Il faut donc trouver une amélioration à cette méthode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_important_neurons(model: nn.Module, important_neurons: Dict[str, torch.Tensor], percentage_masked: float=0.1) -> nn.Module:\n",
    "    masked_model = deepcopy(model)\n",
    "    masked_model.eval()\n",
    "    for layer_name, positive_indices in important_neurons.items():\n",
    "        mask_size = int(positive_indices.shape[0] * percentage_masked)\n",
    "        indices = torch.LongTensor(random.sample(positive_indices.tolist(), k=mask_size))\n",
    "\n",
    "        layer = masked_model.get_submodule(layer_name)\n",
    "        layer_weights = layer.weight.data\n",
    "        layer_biases = layer.bias.data\n",
    "        layer_weights[indices, :] = 0.\n",
    "        layer_biases[indices] = 0.\n",
    "        \n",
    "        layer.weight.data = layer_weights\n",
    "        layer.bias.data = layer_biases\n",
    "    \n",
    "    return masked_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_masked_accuracy = []\n",
    "non_important_masked_accuracy = []\n",
    "for _ in range(10):\n",
    "    important_masked_model = mask_important_neurons(submodel, layers_important_neurons, percentage_masked=0.9)\n",
    "    non_important_masked_model = mask_important_neurons(submodel, layers_non_important_neurons, percentage_masked=0.9)\n",
    "\n",
    "    important_outputs = important_masked_model(X_test)\n",
    "    non_important_outputs = non_important_masked_model(X_test)\n",
    "\n",
    "    important_accuracy = BinaryAccuracy()(important_outputs, y_test.unsqueeze(1))\n",
    "    non_important_accuracy = BinaryAccuracy()(non_important_outputs, y_test.unsqueeze(1))\n",
    "\n",
    "    important_masked_accuracy.append(important_accuracy)\n",
    "    non_important_masked_accuracy.append(non_important_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_masked_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_important_masked_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trouver un modèle plus gros, et commencer à réfléchir aux interactions !\n",
    "\n",
    "Pour les interactions, je pense faire couche par couche afin de noter les changements de distribution d'une couche vers sa suivante, lorsque l'on désactive cette couche. Nous sommes donc dans le cas de la causalité à nous demander ce qu'il se passerait sur la couche suivante si on coupe le signal de la couche précédente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
