{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "import random\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchmetrics.classification import BinaryAccuracy\n",
    "\n",
    "from src.dataset_handler.classic_dataset import ClassicDataset\n",
    "from src.examples.utils_heart import preprocess_heart_data\n",
    "from src.models.lightning_wrapper import LightningWrapper\n",
    "from src.models.multi_linear_layers import MultiLinearLayers\n",
    "from src.neurons_importance import NeuronsImportance\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "Déceler les petits bugs restants, surtout en fin de traitement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir ../lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = '/home/lbaret/projects/explainability_sandbox/lightning_logs/version_0/checkpoints/epoch=99-step=9200.ckpt'\n",
    "\n",
    "df_test = pd.read_csv('../data/test_heart.csv')\n",
    "inputs, labels = df_test_prep = preprocess_heart_data(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = ClassicDataset(inputs, labels)\n",
    "test_loader = DataLoader(test_set, batch_size=8, shuffle=False)\n",
    "\n",
    "network = MultiLinearLayers(inputs.shape[1], 1)\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = LightningWrapper(network, loss_function, metrics={'accuracy': BinaryAccuracy().to('cuda')})\n",
    "model.load_from_checkpoint(checkpoint_path=checkpoints)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bel entrainement maintenant, essayons de comprendre les distributions des neurones de chacune des couches, afin de savoir si c'est uniformément distribués où chaque neurone présente sa spécificité."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récupérons le test set pour pouvoir commencer à travailler dessus. Un modèle simple + un cas binaire pour généraliser ensuite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submodel = deepcopy(model.wrapped_model)\n",
    "submodel.eval()\n",
    "\n",
    "# Autant tout faire passer d'un coup\n",
    "X_test = []\n",
    "y_test = []\n",
    "for x, y in test_set:\n",
    "    X_test.append(x.unsqueeze(0))\n",
    "    y_test.append(y)\n",
    "\n",
    "X_test = torch.cat(X_test)\n",
    "y_test = torch.cat(y_test)\n",
    "\n",
    "# On peut chercher les logits du modèle\n",
    "outputs = submodel(X_test)\n",
    "\n",
    "BinaryAccuracy()(outputs, y_test.unsqueeze(1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super, maintenant entammons une analyse poussée de nos couches en sorties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_neurons_collector = {}\n",
    "in_neurons_collector = {}\n",
    "def forward_hook(module: nn.Module, inputs: torch.Tensor, outputs: torch.Tensor, name: str, out_neurons_collector: Dict[str, List[torch.Tensor]],\n",
    "                 in_neurons_collector: Dict[str, List[torch.Tensor]]) -> None:\n",
    "    if name in out_neurons_collector.keys():\n",
    "        out_neurons_collector[name].append(outputs.detach().cpu())\n",
    "    else:\n",
    "        out_neurons_collector[name] = [outputs.detach().cpu()]\n",
    "    \n",
    "    if name in in_neurons_collector.keys():\n",
    "        in_neurons_collector[name].append(inputs[0].detach().cpu())\n",
    "    else:\n",
    "        in_neurons_collector[name] = [inputs[0].detach().cpu()]\n",
    "\n",
    "hooks = []\n",
    "for name, module in submodel.named_modules():\n",
    "    if name != '':\n",
    "        hooks.append(module.register_forward_hook(partial(forward_hook, name=name, out_neurons_collector=out_neurons_collector, in_neurons_collector=in_neurons_collector)))\n",
    "\n",
    "outputs = submodel(X_test)\n",
    "\n",
    "for h in hooks:\n",
    "    h.remove()\n",
    "\n",
    "for layer_name, neurons in out_neurons_collector.items():\n",
    "    out_neurons_collector[layer_name] = deepcopy(torch.cat(out_neurons_collector[layer_name]))\n",
    "\n",
    "for layer_name, neurons in in_neurons_collector.items():\n",
    "    in_neurons_collector[layer_name] = deepcopy(torch.cat(in_neurons_collector[layer_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_neurons_collector['linear2'].mean(dim=0), out_neurons_collector['linear2'].std(dim=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avons nous à faire à des distributions normales ? Un test statistique pourra nous le dire !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_samples = {}\n",
    "shapiro_pvalues = {}\n",
    "\n",
    "for layer_name, layer_tensor in out_neurons_collector.items():\n",
    "    normal_samples[layer_name] = []\n",
    "    shapiro_pvalues[layer_name] = []\n",
    "\n",
    "    for i in range(layer_tensor.shape[1]):\n",
    "        samples = layer_tensor[:, i]\n",
    "        shapiro_test = stats.shapiro(samples)\n",
    "\n",
    "        normal_samples[layer_name].append(True if shapiro_test.pvalue > 0.05 else False)\n",
    "        shapiro_pvalues[layer_name].append(shapiro_test.pvalue)\n",
    "\n",
    "    normal_samples[layer_name] = torch.BoolTensor(normal_samples[layer_name])\n",
    "    shapiro_pvalues[layer_name] = torch.Tensor(shapiro_pvalues[layer_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "sorted_indices = shapiro_pvalues['linear1'].sort()[1][:10]\n",
    "df_distplot = pd.DataFrame()\n",
    "for c, ind in enumerate(sorted_indices):\n",
    "    sns.distplot(out_neurons_collector['linear1'][:, ind.item()], ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Souvent bimodal, c'est intéressant !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "sorted_indices = shapiro_pvalues['linear1'].sort(descending=True)[1][:10]\n",
    "df_distplot = pd.DataFrame()\n",
    "for c, ind in enumerate(sorted_indices):\n",
    "    sns.distplot(out_neurons_collector['linear1'][:, ind.item()], ax=ax, label=ind)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ont-ils simplement un effet régularisateur ?\n",
    "\n",
    "Dans deux cas, où se situent les valeurs pour la classe 0 et la classe 1 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "sorted_indices = shapiro_pvalues['linear1'].sort()[1][:10]\n",
    "df_distplot = pd.DataFrame()\n",
    "for c, ind in enumerate(sorted_indices):\n",
    "    sns.distplot(out_neurons_collector['linear1'][:, ind.item()], ax=ax, label=ind)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices = shapiro_pvalues['linear1'].sort()[1][:10]\n",
    "\n",
    "df_linear_1_unnormal = pd.DataFrame(data=out_neurons_collector['linear1'][:, sorted_indices])\n",
    "df_linear_1_unnormal['label'] = y_test\n",
    "\n",
    "sns.displot(data=df_linear_1_unnormal.melt(id_vars=['label']), x='value', hue='label', col='variable', col_wrap=3, alpha=0.5, kde=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices = shapiro_pvalues['linear1'].sort(descending=True)[1][:10]\n",
    "\n",
    "df_linear_1_normal = pd.DataFrame(data=out_neurons_collector['linear1'][:, sorted_indices])\n",
    "df_linear_1_normal['label'] = y_test\n",
    "\n",
    "sns.displot(data=df_linear_1_normal.melt(id_vars=['label']), x='value', hue='label', col='variable', col_wrap=3, alpha=0.5, kde=True);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Admettons que nous avons des neurones que l'on considère comme dissociatifs, pouvons nous aisément repérer les poids ou l'ensemble des poids qui vont maintenir ou créer une dissociation par la suite ? \\\n",
    "Si les poids n'accentuent, ne conservent ou ne créent pas de dissociation alors ça va être difficile de repérer les neurones importants. \\\n",
    "Une de mes hypothèses est que les neurones dont les distributions sont confondues ne sont présents qu'à des fins de régulation et ne peuvent être considérés comme importants car non dissociés.\n",
    "\n",
    "La question que nous pouvons nous poser maintenant est : quels sont les neurones qui présentent la plus grosse dissociation ? Que ce soit fortement positif ou négatif. Si présence de ReLU alors la négation est considérée comme nul et alors le signal qui est émit ne vient que d'une classe en soi.\n",
    "\n",
    "Dans la méthode, nous ne pouvons négliger le théorème central limite, bien que nous ayons l'équivalent de 2 variables aléatoires par neurone (l'effet peut être plus remarquable avec plus de classes par exemple). L'union des deux distributions (de la classe 0 et de la classe 1) peut donner une distribution normale. Donc, il serait intéressant de mesurer 2 valeurs :\n",
    "1. La divergence de *Kullback-Leibler* (sûrement le plus intéressant)\n",
    "2. La moyenne et l'écart-type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.random.normal(loc=5.0, scale=1.5, size=(10000,))\n",
    "X2 = np.random.normal(loc=5.0, scale=1.5, size=(1000,))\n",
    "X3 = np.random.normal(loc=3.0, scale=1.0, size=(50,))\n",
    "X4 = np.random.normal(loc=5.0, scale=2.0, size=(10000,))\n",
    "\n",
    "stats.mannwhitneyu(X1, X2), stats.mannwhitneyu(X1, X3), stats.mannwhitneyu(X1, X4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_positive_1 = df_linear_1_normal[df_linear_1_normal['label'] == 1.][1].to_numpy()\n",
    "X_negative_1 = df_linear_1_normal[df_linear_1_normal['label'] == 0.][1].to_numpy()\n",
    "\n",
    "X_positive_9 = df_linear_1_normal[df_linear_1_normal['label'] == 1.][9].to_numpy()\n",
    "X_negative_9 = df_linear_1_normal[df_linear_1_normal['label'] == 0.][9].to_numpy()\n",
    "\n",
    "stats.mannwhitneyu(X_positive_1, X_negative_1), stats.mannwhitneyu(X_positive_9, X_negative_9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisons ce test statistique pour déterminer l'importance d'un neurone. Procédons par étapes :\n",
    "1. Récupérons le dictionnaire des outputs d'une couche\n",
    "2. Séparons les valeurs par rapport à leur classe\n",
    "3. Effectuons les tests statistiques suivants :\n",
    "   1. Si distributions normales ou proches : t-test (ou Student)\n",
    "   2. Sinon : test de Mann-Whitney U\n",
    "      1. Si semblables : cherchons voir si la moyenne/variance est similaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_indices = torch.where(y_test == 0)[0]\n",
    "positive_indices = torch.where(y_test == 1)[0]\n",
    "\n",
    "layers_important_neurons = {}\n",
    "layers_non_important_neurons = {}\n",
    "\n",
    "for layer_name, layer_outputs in out_neurons_collector.items():\n",
    "    if layer_name == 'fc':\n",
    "        continue\n",
    "    linear1 = out_neurons_collector['linear1']\n",
    "\n",
    "    important_neurons = torch.zeros(size=(layer_outputs.shape[1],), dtype=torch.bool)\n",
    "\n",
    "    for i in range(layer_outputs.shape[1]):\n",
    "        samples = layer_outputs[:, i]\n",
    "        important_neurons[i] = torch.BoolTensor([NeuronsImportance.neuron_is_important(samples, positive_indices, negative_indices)])[0]\n",
    "\n",
    "    layers_important_neurons[layer_name] = torch.where(important_neurons == True)[0]\n",
    "    layers_non_important_neurons[layer_name] = torch.where(important_neurons == False)[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour tester notre méthode, prenons aléatoirement et en répétant 10 fois, des neurones importants, effacons les (à 0) puis évaluons la perte en performance (faire pareil avec les neurones pas importants)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_important_neurons"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour masquer les neurones il faut masquer tous les poids et biais arrivant à ce neurone. Ce faisant nous empêchons tout signal d'arriver jusqu'à ce neurone et donc il ne sera pas utile pour la suite des traitements.\n",
    "\n",
    "Cependant ce qu'il faut prendre en compte, c'est que cette méthode statistique ne permet pas de prendre en compte toutes les interactions entre les neurones des couches successives. Donc éteindre le signal provenant d'un neurone peut avoir une conséquence dans la couche suivante. Il faut donc trouver une amélioration à cette méthode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_masked_accuracy = []\n",
    "non_important_masked_accuracy = []\n",
    "for _ in range(10):\n",
    "    important_masked_model = NeuronsImportance.mask_important_neurons(submodel, layers_important_neurons, percentage_masked=0.9)\n",
    "    non_important_masked_model = NeuronsImportance.mask_important_neurons(submodel, layers_non_important_neurons, percentage_masked=0.9)\n",
    "\n",
    "    important_outputs = important_masked_model(X_test)\n",
    "    non_important_outputs = non_important_masked_model(X_test)\n",
    "\n",
    "    important_accuracy = BinaryAccuracy()(important_outputs, y_test.unsqueeze(1))\n",
    "    non_important_accuracy = BinaryAccuracy()(non_important_outputs, y_test.unsqueeze(1))\n",
    "\n",
    "    important_masked_accuracy.append(important_accuracy)\n",
    "    non_important_masked_accuracy.append(non_important_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_masked_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_important_masked_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trouver un modèle plus gros, et commencer à réfléchir aux interactions !\n",
    "\n",
    "Pour les interactions, je pense faire couche par couche afin de noter les changements de distribution d'une couche vers sa suivante, lorsque l'on désactive cette couche. Nous sommes donc dans le cas de la causalité à nous demander ce qu'il se passerait sur la couche suivante si on coupe le signal de la couche précédente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traitement par rapport aux distributions\n",
    "\n",
    "Afin de simplifier l'approche, commençons par traiter les données en exploitant les quantiles.\n",
    "Récupérons les quantiles suivants :\n",
    "1. 0.05 -> Valeur extrême faible (on pourrait récupérer 0.25 à la place)\n",
    "2. 0.5 -> Valeur moyenne\n",
    "3. 0.95 -> Valeur extrême forte (on pourrait récupérer 0.75 à la place)\n",
    "   \n",
    "=> Une fois chacune des valeurs étudiées en sortie du réseau de neurones, nous pouvons dresser un intervalle de variation sur les valeurs que peuvent prendre la sortie. Si nous procédons de la sortie vers l'entrée du réseau, nous pouvons déterminer un chemin de neurones idéal. Pourquoi procéder ainsi ? Car il est plus logique d'étudier les interactions entre les neurones directes, comme si nous étions dans le cadre d'un graphe. Les neurones précédents influencants directement les valeurs des neurones suivants.\n",
    "\n",
    "Pour procéder, découpons le traitement :\n",
    "1. Définir un hook qui prend en input le dictionnaire du tenseur des quantiles et des valeurs à mettre à zéro.\n",
    "2. À chaque itération, nous reculons d'une couche en donnant les indices des neurones ayant été impactés à l'itération précédente.\n",
    "3. Nous faisons ainsi varier les valeurs de neurones en entrée qui vont directement impacter les neurones déterminés à l'itération précédente.\n",
    "4. Nous répétons ces étapes afin de dresser les chemins de neurones optimaux.\n",
    "\n",
    "De cette façon, nous aurons une chance de découvrir les caractéristiques essentielles du jeu de données offrant la plus grosse variation de la sortie pour une classe donnée, bien que nous commençons par le cas simple binaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layers_ordered(module: nn.Module, module_name: str=None) -> List[nn.Module]:\n",
    "    \"\"\"\n",
    "        Get ordered layers in the network as a list of tuples containing : (<name of layer as str>, <layer module as nn.Module>)\n",
    "    \"\"\"\n",
    "    children_modules = list(module.named_children())\n",
    "    leaves_modules = []\n",
    "    for child_name, child_mod in children_modules:\n",
    "        leaves_modules += get_layers_ordered(child_mod, child_name)\n",
    "    \n",
    "    if len(children_modules) == 0 and len(list(module.parameters())) > 0:\n",
    "        return [(module_name, module)]\n",
    "    \n",
    "    return leaves_modules\n",
    "    \n",
    "layers = get_layers_ordered(model)\n",
    "layers.reverse()\n",
    "\n",
    "all_quantiles = [0., 0.05, 0.25, 0.5, 0.75, 0.95, 1.]\n",
    "quantiles_by_layers = {}\n",
    "for layer_name, layer_inputs in in_neurons_collector.items():\n",
    "    quantiles_by_layers[layer_name] = {\n",
    "        q: torch.quantile(layer_inputs, q=torch.scalar_tensor(q), dim=0)\n",
    "        for q in all_quantiles\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_hook_pre_forward(module: nn.Module, inputs: Tuple[torch.Tensor], remplacement_tensor: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    return (remplacement_tensor * mask, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "variations = []\n",
    "with torch.no_grad():\n",
    "    for ni in range(layers[0][1].in_features):\n",
    "        for q in all_quantiles:\n",
    "            mask = torch.zeros(size=(layers[0][1].in_features,))\n",
    "            mask[ni] = 1\n",
    "            \n",
    "            h = layers[0][1].register_forward_pre_hook(partial(quantile_hook_pre_forward, remplacement_tensor=quantiles_by_layers[layers[0][0]][q], mask=mask))\n",
    "            out = model(torch.zeros(size=(1, 17)))\n",
    "            variations.append(out)\n",
    "\n",
    "            h.remove()\n",
    "\n",
    "variations = torch.cat(variations)\n",
    "variations = variations.reshape(layers[0][1].in_features, len(all_quantiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _quantile_hook_pre_forward(module: nn.Module, inputs: Tuple[torch.Tensor], remplacement_tensor: torch.Tensor,\n",
    "                                  mask: torch.Tensor) -> torch.Tensor:\n",
    "        return (remplacement_tensor * mask, )\n",
    "\n",
    "class DistributionNeuronsImportance:\n",
    "    \"\"\"\n",
    "        Works only with straightforward models. Parallel processing into the model is not handled (e.g. ResNet downsampling in parallel with block process).\n",
    "    \"\"\"\n",
    "    def __init__(self, network: nn.Module, num_features: int, target_label: int, in_neurons_collector: Dict[str, torch.Tensor], target_layer_name: str=None, \n",
    "                 process_quantiles: List[float]=[0., 0.05, 0.25, 0.5, 0.75, 0.95, 1.], positive_minimum_quantile: float=0.95, \n",
    "                 negative_maximum_quantile: float=0.05) -> None:\n",
    "        self.network = network\n",
    "        self.network.eval()\n",
    "\n",
    "        self.num_features = num_features\n",
    "        self.target_label = target_label\n",
    "\n",
    "        self.positive_minimum_quantile = positive_minimum_quantile\n",
    "        self.negative_maximum_quantile = negative_maximum_quantile\n",
    "\n",
    "        self.target_layer_name = None\n",
    "        self.target_layer_module = None\n",
    "        self._set_target_layer_infos(target_layer_name)\n",
    "\n",
    "        layers = self.get_layers_ordered(model)\n",
    "        layers.reverse()\n",
    "        self.layers = layers\n",
    "\n",
    "        self.process_quantiles = process_quantiles\n",
    "        self.in_neurons_collector = in_neurons_collector\n",
    "\n",
    "        self.quantiles_by_layer = self._set_quantiles_by_layers()\n",
    "\n",
    "    def _set_target_layer_infos(self, target_layer_name: str) -> nn.Module:\n",
    "        if target_layer_name is None:\n",
    "            name, mod = [(name, mod) for name, mod in self.network.named_modules()][-1]\n",
    "        else:\n",
    "            mod = self.network.get_submodule(target_layer_name)\n",
    "            name = target_layer_name\n",
    "\n",
    "        self.target_layer_name = name\n",
    "        self.target_layer_module = mod\n",
    "\n",
    "    def _set_quantiles_by_layers(self) -> Dict[float, torch.Tensor]:\n",
    "        quantiles_by_layers = {}\n",
    "        for layer_name, layer_inputs in self.in_neurons_collector.items():\n",
    "            quantiles_by_layers[layer_name] = {\n",
    "                q: torch.quantile(layer_inputs, q=torch.scalar_tensor(q), dim=0)\n",
    "                for q in self.process_quantiles\n",
    "            }\n",
    "        \n",
    "        return quantiles_by_layers\n",
    "    \n",
    "    def get_layers_ordered(self, module: nn.Module, module_name: str=None) -> List[nn.Module]:\n",
    "        \"\"\"\n",
    "            Get ordered layers in the network as a list of tuples containing : (<name of layer as str>, <layer module as nn.Module>)\n",
    "        \"\"\"\n",
    "        children_modules = list(module.named_children())\n",
    "        leaves_modules = []\n",
    "        for child_name, child_mod in children_modules:\n",
    "            leaves_modules += get_layers_ordered(child_mod, child_name)\n",
    "        \n",
    "        if len(children_modules) == 0 and len(list(module.parameters())) > 0:\n",
    "            return [(module_name, module)]\n",
    "        \n",
    "        return leaves_modules\n",
    "    \n",
    "    def _get_positive_indices_from_variations(self, variations: torch.Tensor) -> torch.Tensor:\n",
    "        variations_of_outputs = variations[:, len(self.process_quantiles)-1] - variations[:, 0]\n",
    "\n",
    "        positive_indices_variations = torch.where(variations_of_outputs > 0)[0]\n",
    "        positive_quantile_threshold = torch.quantile(variations_of_outputs[positive_indices_variations], q=self.positive_minimum_quantile).item()\n",
    "        best_positive_variations_of_outputs = torch.where(variations_of_outputs >= positive_quantile_threshold)[0]\n",
    "        best_positive_variations_of_outputs = best_positive_variations_of_outputs.sort(descending=False).values\n",
    "\n",
    "        return best_positive_variations_of_outputs\n",
    "    \n",
    "    def _get_negative_indices_from_variations(self, variations: torch.Tensor) -> torch.Tensor:\n",
    "        variations_of_outputs = variations[:, len(self.process_quantiles)-1] - variations[:, 0]\n",
    "\n",
    "        negative_indices_variations = torch.where(variations_of_outputs < 0)[0]\n",
    "        negative_quantile_threshold = torch.quantile(variations_of_outputs[negative_indices_variations], q=self.negative_maximum_quantile).item()\n",
    "        best_negative_variations_of_outputs = torch.where(variations_of_outputs <= negative_quantile_threshold)[0]\n",
    "        best_negative_variations_of_outputs = best_negative_variations_of_outputs.sort(descending=False).values\n",
    "\n",
    "        return best_negative_variations_of_outputs\n",
    "\n",
    "    def _get_hooked_results_model(self, layer_module: nn.Module, replacement_tensor: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        hook = layer_module.register_forward_pre_hook(\n",
    "            partial(_quantile_hook_pre_forward, remplacement_tensor=replacement_tensor, mask=mask)\n",
    "        )\n",
    "        out = self.network(torch.zeros(size=(1, self.num_features)))\n",
    "        hook.remove()\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def _compute_layer_variations_search(self, layer_module: nn.Module, module_name: str, previous_positive_neurons: torch.Tensor,\n",
    "                                         previous_negative_neurons: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        positive_variations = []\n",
    "        negative_variations = []\n",
    "        with torch.no_grad():\n",
    "            for ni in range(layer_module.in_features):\n",
    "                for q in self.process_quantiles:\n",
    "                    positive_mask = torch.zeros(size=(layer_module.out_features, layer_module.in_features))\n",
    "                    positive_mask[previous_positive_neurons, ni] = 1\n",
    "\n",
    "                    positive_variations.append(\n",
    "                        self._get_hooked_results_model(layer_module, self.quantiles_by_layer[module_name][q], positive_mask)\n",
    "                    )\n",
    "\n",
    "                    negative_mask = torch.zeros(size=(layer_module.out_features, layer_module.in_features))\n",
    "                    negative_mask[previous_negative_neurons, ni] = 1\n",
    "\n",
    "                    negative_variations.append(\n",
    "                        self._get_hooked_results_model(layer_module, self.quantiles_by_layer[module_name][q], negative_mask)\n",
    "                    )\n",
    "        \n",
    "        return positive_variations, negative_variations\n",
    "    \n",
    "    def get_neurons_importance_indices(self) -> Dict[str, Dict[str, torch.Tensor]]:      \n",
    "        \"\"\" Iterate over network layers to compute impotant neurons indices which have positive and negative effects on the target label.\n",
    "            Furthermore, only nn.Linear layers are handled for the moment.\n",
    "        \n",
    "        :param test: _description_\n",
    "        :type test: None\n",
    "        :return: positive and negative effects indices for each layers\n",
    "        :rtype: Dict[str, Dict[str, torch.Tensor]]\n",
    "        \"\"\"\n",
    "        previous_positive_neurons = torch.tensor([self.target_label], dtype=torch.long)\n",
    "        previous_negative_neurons = torch.tensor([self.target_label], dtype=torch.long)\n",
    "\n",
    "        important_neurons_indices = {}\n",
    "        for name, module in self.layers:\n",
    "            print(previous_positive_neurons.shape)\n",
    "            positive_variations, negative_variations = self._compute_layer_variations_search(\n",
    "                module, name, previous_positive_neurons, previous_negative_neurons\n",
    "            )\n",
    "\n",
    "            positive_variations = torch.cat(positive_variations)\n",
    "            positive_variations = positive_variations.reshape(module.in_features, self.target_layer_module.out_features, len(self.process_quantiles))[:, self.target_label, :]\n",
    "\n",
    "            negative_variations = torch.cat(negative_variations)\n",
    "            negative_variations = negative_variations.reshape(module.in_features, self.target_layer_module.out_features, len(self.process_quantiles))[:, self.target_label, :]\n",
    "            \n",
    "            best_positive_variations_of_outputs = self._get_positive_indices_from_variations(positive_variations)\n",
    "            best_negative_variations_of_outputs = self._get_positive_indices_from_variations(negative_variations)\n",
    "            \n",
    "            important_neurons_indices[name] = {\n",
    "                'positive': best_positive_variations_of_outputs,\n",
    "                'negative': best_negative_variations_of_outputs\n",
    "            }\n",
    "\n",
    "            previous_positive_neurons = best_positive_variations_of_outputs.clone()\n",
    "            previous_negative_neurons = best_negative_variations_of_outputs.clone()\n",
    "\n",
    "        return important_neurons_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "512*7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distrib_neur_imp = DistributionNeuronsImportance(model.wrapped_model, 17, 0, in_neurons_collector, 'fc')\n",
    "distrib_neur_imp.get_neurons_importance_indices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1024*512*7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
