{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_fruits_vegetables.ipynb","provenance":[],"collapsed_sections":["tl6upF1runT2"],"authorship_tag":"ABX9TyNimNnyQTsjnINR3+w4I9jH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"d4277f7cdb2e4d4a80d24e674c9dc97b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_354a3e7f642446c7844d9c7ce630458a","IPY_MODEL_782234c8128b4c4a86e757781ce979dc","IPY_MODEL_5f9c703c624f4221a03b30503b319fba"],"layout":"IPY_MODEL_53da8d2aab0541d7ad0f7056c5101968"}},"354a3e7f642446c7844d9c7ce630458a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_454036a7cb244b85a1a8be9ce1b60526","placeholder":"​","style":"IPY_MODEL_c12f47f45f8e4fb2bbef012b2ba23113","value":"100%"}},"782234c8128b4c4a86e757781ce979dc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_458a5e3c61d6452380b540f4a5c44668","max":102530333,"min":0,"orientation":"horizontal","style":"IPY_MODEL_53445eeaa8024904a52ef3e851172ad6","value":102530333}},"5f9c703c624f4221a03b30503b319fba":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b56d05245854a81af77d9a7fae503a5","placeholder":"​","style":"IPY_MODEL_1e760c37bc0a43df803cefa7efa263ae","value":" 97.8M/97.8M [00:01&lt;00:00, 59.0MB/s]"}},"53da8d2aab0541d7ad0f7056c5101968":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"454036a7cb244b85a1a8be9ce1b60526":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c12f47f45f8e4fb2bbef012b2ba23113":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"458a5e3c61d6452380b540f4a5c44668":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53445eeaa8024904a52ef3e851172ad6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8b56d05245854a81af77d9a7fae503a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e760c37bc0a43df803cefa7efa263ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Mounting Drive and installing missing librairies"],"metadata":{"id":"apBBZuNFtSrx"}},{"cell_type":"code","execution_count":6,"metadata":{"id":"Qwy0vNKStCH-","executionInfo":{"status":"ok","timestamp":1661631509163,"user_tz":240,"elapsed":19942,"user":{"displayName":"Lobiten AI-ML","userId":"13610020337788558431"}}},"outputs":[],"source":["%%capture\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","!pip install pytorch_lightning\n","!pip install ipywidgets\n","!pip install rich\n","!pip install captum"]},{"cell_type":"markdown","source":["# Notebook settings"],"metadata":{"id":"uqBfglz8lwnW"}},{"cell_type":"code","source":["do_training = True"],"metadata":{"id":"is5Z5qnJly1t","executionInfo":{"status":"ok","timestamp":1661631530601,"user_tz":240,"elapsed":178,"user":{"displayName":"Lobiten AI-ML","userId":"13610020337788558431"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"6EWfvpjBuJlX"}},{"cell_type":"code","source":["from zipfile import ZipFile\n","\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import random_split, DataLoader\n","import torchvision.transforms as transforms\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torchvision.models import ResNet50_Weights\n","# Voir si c'est torch ou torchvision à ce niveau\n","from torchvision.utils import _log_api_usage_once\n","from torchvision.models._api import WeightsEnum, Weights\n","from torchvision.models._meta import _IMAGENET_CATEGORIES\n","from torchvision.models._utils import handle_legacy_interface, _ovewrite_named_param\n","\n","import pytorch_lightning as pl\n","\n","from typing import Any, Dict, Callable, Tuple, Optional, Type, Union, List\n","\n","from pytorch_lightning.callbacks import RichProgressBar, ModelCheckpoint\n","from pytorch_lightning.callbacks.progress.rich_progress import RichProgressBarTheme\n","\n","# from captum.attr import DeepLift, DeepLiftShap, GradientShap, Saliency\n","import captum.attr as explain"],"metadata":{"id":"c5d9WkLLuFcG","executionInfo":{"status":"ok","timestamp":1661631532331,"user_tz":240,"elapsed":293,"user":{"displayName":"Lobiten AI-ML","userId":"13610020337788558431"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# Torchvision model redefining"],"metadata":{"id":"sinye0pD4rON"}},{"cell_type":"code","source":["def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n","    \"\"\"3x3 convolution with padding\"\"\"\n","    return nn.Conv2d(\n","        in_planes,\n","        out_planes,\n","        kernel_size=3,\n","        stride=stride,\n","        padding=dilation,\n","        groups=groups,\n","        bias=False,\n","        dilation=dilation,\n","    )\n","\n","\n","def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n","    \"\"\"1x1 convolution\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion: int = 1\n","\n","    def __init__(\n","        self,\n","        inplanes: int,\n","        planes: int,\n","        stride: int = 1,\n","        downsample: Optional[nn.Module] = None,\n","        groups: int = 1,\n","        base_width: int = 64,\n","        dilation: int = 1,\n","        norm_layer: Optional[Callable[..., nn.Module]] = None,\n","    ) -> None:\n","        super().__init__()\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        if groups != 1 or base_width != 64:\n","            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n","        if dilation > 1:\n","            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n","        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n","        self.conv1 = conv3x3(inplanes, planes, stride)\n","        self.bn1 = norm_layer(planes)\n","        self.relu = nn.ReLU()\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = norm_layer(planes)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        identity = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            identity = self.downsample(x)\n","\n","        out += identity\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n","    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n","    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n","    # This variant is also known as ResNet V1.5 and improves accuracy according to\n","    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n","\n","    expansion: int = 4\n","\n","    def __init__(\n","        self,\n","        inplanes: int,\n","        planes: int,\n","        stride: int = 1,\n","        downsample: Optional[nn.Module] = None,\n","        groups: int = 1,\n","        base_width: int = 64,\n","        dilation: int = 1,\n","        norm_layer: Optional[Callable[..., nn.Module]] = None,\n","    ) -> None:\n","        super().__init__()\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        width = int(planes * (base_width / 64.0)) * groups\n","        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n","        self.conv1 = conv1x1(inplanes, width)\n","        self.bn1 = norm_layer(width)\n","        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n","        self.bn2 = norm_layer(width)\n","        self.conv3 = conv1x1(width, planes * self.expansion)\n","        self.bn3 = norm_layer(planes * self.expansion)\n","        self.relu = nn.ReLU()\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        identity = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        if self.downsample is not None:\n","            identity = self.downsample(x)\n","\n","        out += identity\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(\n","        self,\n","        block: Type[Union[BasicBlock, Bottleneck]],\n","        layers: List[int],\n","        num_classes: int = 1000,\n","        zero_init_residual: bool = False,\n","        groups: int = 1,\n","        width_per_group: int = 64,\n","        replace_stride_with_dilation: Optional[List[bool]] = None,\n","        norm_layer: Optional[Callable[..., nn.Module]] = None,\n","    ) -> None:\n","        super().__init__()\n","        _log_api_usage_once(self)\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        self._norm_layer = norm_layer\n","\n","        self.inplanes = 64\n","        self.dilation = 1\n","        if replace_stride_with_dilation is None:\n","            # each element in the tuple indicates if we should replace\n","            # the 2x2 stride with a dilated convolution instead\n","            replace_stride_with_dilation = [False, False, False]\n","        if len(replace_stride_with_dilation) != 3:\n","            raise ValueError(\n","                \"replace_stride_with_dilation should be None \"\n","                f\"or a 3-element tuple, got {replace_stride_with_dilation}\"\n","            )\n","        self.groups = groups\n","        self.base_width = width_per_group\n","        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n","        self.bn1 = norm_layer(self.inplanes)\n","        self.relu = nn.ReLU()\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","        self.layer1 = self._make_layer(block, 64, layers[0])\n","        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])\n","        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\n","        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.fc = nn.Linear(512 * block.expansion, num_classes)\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n","            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","\n","        # Zero-initialize the last BN in each residual branch,\n","        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n","        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n","        if zero_init_residual:\n","            for m in self.modules():\n","                if isinstance(m, Bottleneck) and m.bn3.weight is not None:\n","                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n","                elif isinstance(m, BasicBlock) and m.bn2.weight is not None:\n","                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n","\n","    def _make_layer(\n","        self,\n","        block: Type[Union[BasicBlock, Bottleneck]],\n","        planes: int,\n","        blocks: int,\n","        stride: int = 1,\n","        dilate: bool = False,\n","    ) -> nn.Sequential:\n","        norm_layer = self._norm_layer\n","        downsample = None\n","        previous_dilation = self.dilation\n","        if dilate:\n","            self.dilation *= stride\n","            stride = 1\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                conv1x1(self.inplanes, planes * block.expansion, stride),\n","                norm_layer(planes * block.expansion),\n","            )\n","\n","        layers = []\n","        layers.append(\n","            block(\n","                self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer\n","            )\n","        )\n","        self.inplanes = planes * block.expansion\n","        for _ in range(1, blocks):\n","            layers.append(\n","                block(\n","                    self.inplanes,\n","                    planes,\n","                    groups=self.groups,\n","                    base_width=self.base_width,\n","                    dilation=self.dilation,\n","                    norm_layer=norm_layer,\n","                )\n","            )\n","\n","        return nn.Sequential(*layers)\n","\n","    def _forward_impl(self, x: torch.Tensor) -> torch.Tensor:\n","        # See note [TorchScript super()]\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.fc(x)\n","\n","        return x\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        return self._forward_impl(x)\n","\n","\n","def _resnet(\n","    block: Type[Union[BasicBlock, Bottleneck]],\n","    layers: List[int],\n","    weights: Optional[WeightsEnum],\n","    progress: bool,\n","    **kwargs: Any,\n",") -> ResNet:\n","    if weights is not None:\n","        _ovewrite_named_param(kwargs, \"num_classes\", len(weights.meta[\"categories\"]))\n","\n","    model = ResNet(block, layers, **kwargs)\n","\n","    if weights is not None:\n","        model.load_state_dict(weights.get_state_dict(progress=progress))\n","\n","    return model\n","\n","\n","_COMMON_META = {\n","    \"min_size\": (1, 1),\n","    \"categories\": _IMAGENET_CATEGORIES,\n","}\n","\n","def resnet50(*, weights: Optional[ResNet50_Weights] = None, progress: bool = True, **kwargs: Any) -> ResNet:\n","    \"\"\"ResNet-50 from `Deep Residual Learning for Image Recognition <https://arxiv.org/pdf/1512.03385.pdf>`__.\n","\n","    .. note::\n","       The bottleneck of TorchVision places the stride for downsampling to the second 3x3\n","       convolution while the original paper places it to the first 1x1 convolution.\n","       This variant improves the accuracy and is known as `ResNet V1.5\n","       <https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch>`_.\n","\n","    Args:\n","        weights (:class:`~torchvision.models.ResNet50_Weights`, optional): The\n","            pretrained weights to use. See\n","            :class:`~torchvision.models.ResNet50_Weights` below for\n","            more details, and possible values. By default, no pre-trained\n","            weights are used.\n","        progress (bool, optional): If True, displays a progress bar of the\n","            download to stderr. Default is True.\n","        **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet``\n","            base class. Please refer to the `source code\n","            <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_\n","            for more details about this class.\n","\n","    .. autoclass:: torchvision.models.ResNet50_Weights\n","        :members:\n","    \"\"\"\n","    weights = ResNet50_Weights.verify(weights)\n","\n","    return _resnet(Bottleneck, [3, 4, 6, 3], weights, progress, **kwargs)"],"metadata":{"id":"_PC4kxM34tfS","executionInfo":{"status":"ok","timestamp":1661631536707,"user_tz":240,"elapsed":347,"user":{"displayName":"Lobiten AI-ML","userId":"13610020337788558431"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["# Extract and load data"],"metadata":{"id":"tl6upF1runT2"}},{"cell_type":"markdown","source":["## Extract and load train/test sets"],"metadata":{"id":"tdR6VjORx1zr"}},{"cell_type":"code","source":["archive_path = '/content/gdrive/MyDrive/datasets/fruits_vegetables_360.zip'\n","\n","with ZipFile(archive_path, mode='r') as zip:\n","  zip.extractall('/content/fruits_vegetables_360/')\n","\n","train_set = ImageFolder('/content/fruits_vegetables_360/fruits-360_dataset/fruits-360/Training/', transform=transforms.ToTensor())\n","test_set = ImageFolder('/content/fruits_vegetables_360/fruits-360_dataset/fruits-360/Test/', transform=transforms.ToTensor())"],"metadata":{"id":"AOWcRZLBuFeZ","executionInfo":{"status":"ok","timestamp":1661631591396,"user_tz":240,"elapsed":45064,"user":{"displayName":"Lobiten AI-ML","userId":"13610020337788558431"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["## Split training set into training and validation sets"],"metadata":{"id":"bsxNaRGgx5FO"}},{"cell_type":"code","source":["train_ratio = 0.9\n","total_size = len(train_set)\n","\n","train_size = int(train_ratio * total_size)\n","valid_size = total_size - train_size\n","\n","train_set, valid_set = random_split(train_set, [train_size, valid_size])"],"metadata":{"id":"LtdqjFu-uFhK","executionInfo":{"status":"ok","timestamp":1661631591396,"user_tz":240,"elapsed":20,"user":{"displayName":"Lobiten AI-ML","userId":"13610020337788558431"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["Look at set sizes"],"metadata":{"id":"MyoztCvMyi2_"}},{"cell_type":"code","source":["len(train_set), len(valid_set), len(test_set)"],"metadata":{"id":"8dxVwj6lvmJY","executionInfo":{"status":"ok","timestamp":1661631591397,"user_tz":240,"elapsed":7,"user":{"displayName":"Lobiten AI-ML","userId":"13610020337788558431"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f889d162-ba5e-4b94-d281-d9e1be16761e"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(60922, 6770, 22688)"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["# ResNet\n","Simple to use and simple to use explainability process on it."],"metadata":{"id":"5o9bXcLn0eFq"}},{"cell_type":"markdown","source":["## Load and freeze weights for finetuning\n","\n","We freeze the entire model weights and biases just to keep classifier learnable.  \n","We want to keep initial features extractor."],"metadata":{"id":"Oyn4ApAN3onJ"}},{"cell_type":"code","source":["model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n","\n","model.fc = nn.Linear(in_features=2048, out_features=131)\n","\n","for name, params in model.named_parameters():\n","  if 'fc' in name:\n","    continue\n","  params.requires_grad = False"],"metadata":{"id":"GzmPXsJuvmQR","executionInfo":{"status":"ok","timestamp":1661631594722,"user_tz":240,"elapsed":3330,"user":{"displayName":"Lobiten AI-ML","userId":"13610020337788558431"}},"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["d4277f7cdb2e4d4a80d24e674c9dc97b","354a3e7f642446c7844d9c7ce630458a","782234c8128b4c4a86e757781ce979dc","5f9c703c624f4221a03b30503b319fba","53da8d2aab0541d7ad0f7056c5101968","454036a7cb244b85a1a8be9ce1b60526","c12f47f45f8e4fb2bbef012b2ba23113","458a5e3c61d6452380b540f4a5c44668","53445eeaa8024904a52ef3e851172ad6","8b56d05245854a81af77d9a7fae503a5","1e760c37bc0a43df803cefa7efa263ae"]},"outputId":"09b52a0e-9647-4295-c71c-8425ec2ffaa1"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/97.8M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4277f7cdb2e4d4a80d24e674c9dc97b"}},"metadata":{}}]},{"cell_type":"markdown","source":["# PyTorch Lightning wrapper"],"metadata":{"id":"D54JExwR5VTc"}},{"cell_type":"code","source":["class LightningWrapper(pl.LightningModule):\n","  def __init__(self, model: nn.Module, loss_function: Callable=F.cross_entropy, optimizer: torch.optim.Optimizer=torch.optim.Adam, \n","               optimizer_params: Dict[str, Any]={'lr': 0.001}, **pl_module) -> None:\n","    super(LightningWrapper, self).__init__(**pl_module)\n","    self.save_hyperparameters()\n","    self.wrapped_model = model\n","    self.loss_function = loss_function\n","    self.optimizer = optimizer\n","    self.optimizer_params = optimizer_params\n","\n","  def configure_optimizers(self) -> torch.optim.Optimizer:\n","    return self.optimizer(self.parameters(), **self.optimizer_params)\n","  \n","  def forward(self, x: torch.Tensor) -> torch.Tensor:\n","    return self.wrapped_model(x)\n","  \n","  def training_step(self, train_batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> torch.Tensor:\n","    inputs, targets = train_batch\n","    outputs = self.wrapped_model(inputs)\n","    loss = self.loss_function(outputs, targets)\n","    self.log('train_loss', loss)\n","    return loss\n","  \n","  def validation_step(self, valid_batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> torch.Tensor:\n","    inputs, targets = valid_batch\n","    outputs = self.wrapped_model(inputs)\n","    loss = self.loss_function(outputs, targets)\n","    self.log('valid_loss', loss)\n","    return loss\n","  \n","  def test_step(self, valid_batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int) -> torch.Tensor:\n","    inputs, targets = valid_batch\n","    outputs = self.wrapped_model(inputs)\n","    loss = self.loss_function(outputs, targets)\n","    self.log('test_loss', loss)\n","    return loss\n","  \n","  def predict_step(self, batch: torch.Tensor, batch_idx: int) -> torch.Tensor:\n","    inputs = batch\n","    outputs = self.wrapped_model(inputs)\n","    return torch.softmax(outputs)"],"metadata":{"id":"NplFe0Vb5U2k","executionInfo":{"status":"ok","timestamp":1661631594723,"user_tz":240,"elapsed":4,"user":{"displayName":"Lobiten AI-ML","userId":"13610020337788558431"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["Config Lightning Trainer"],"metadata":{"id":"BKx9jwy7XXFy"}},{"cell_type":"code","source":["progress_bar = RichProgressBar(\n","    theme=RichProgressBarTheme(\n","      description=\"green_yellow\",\n","      progress_bar=\"green1\",\n","      progress_bar_finished=\"green1\",\n","      progress_bar_pulse=\"#6206E0\",\n","      batch_progress=\"green_yellow\",\n","      time=\"grey82\",\n","      processing_speed=\"grey82\",\n","      metrics=\"grey82\",\n","    )\n",")\n","\n","model_checkpoint = ModelCheckpoint(\n","    dirpath='/content/gdrive/MyDrive/lightning_logs/finetuning_resnet50_fv/',\n","    save_last=True,\n","    every_n_epochs=1\n",")"],"metadata":{"id":"Wr5MrBEjXWoK","executionInfo":{"status":"ok","timestamp":1661631604255,"user_tz":240,"elapsed":552,"user":{"displayName":"Lobiten AI-ML","userId":"13610020337788558431"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"aOdRqMfb5mVA"}},{"cell_type":"code","source":["# Model and trainer\n","lightning_model = LightningWrapper(model) if do_training else LightningWrapper.load_from_checkpoint('/content/gdrive/MyDrive/lightning_logs/finetuning_resnet50_fv/last.ckpt')\n","\n","device = 'gpu' if torch.cuda.is_available() else 'cpu'\n","trainer = pl.Trainer(accelerator=device, max_epochs=50, callbacks=[progress_bar, model_checkpoint])\n","\n","# Dataloaders\n","train_loader = DataLoader(train_set, batch_size=512, num_workers=2)\n","valid_loader = DataLoader(valid_set, batch_size=512, num_workers=2)\n","test_loader = DataLoader(test_set, batch_size=512, num_workers=2)\n","\n","if do_training:\n","  # Training\n","  trainer.fit(lightning_model, train_dataloaders=train_loader, val_dataloaders=valid_loader)"],"metadata":{"id":"a-HG1P9b4zor","executionInfo":{"status":"aborted","timestamp":1661630965005,"user_tz":240,"elapsed":6,"user":{"displayName":"Lobiten AI-ML","userId":"13610020337788558431"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Testing"],"metadata":{"id":"itrT1czilGjI"}},{"cell_type":"code","source":["trainer.test(lightning_model, dataloaders=test_loader)"],"metadata":{"id":"E2xJvrlPpCIF","executionInfo":{"status":"aborted","timestamp":1661630965005,"user_tz":240,"elapsed":6,"user":{"displayName":"Lobiten AI-ML","userId":"13610020337788558431"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Explicability / Interpretability with Captum"],"metadata":{"id":"TMYQ5LralJUH"}},{"cell_type":"code","source":["img, tgt = train_set[0]\n","img = img.unsqueeze(0)\n","img.requires_grad = True"],"metadata":{"id":"mXfRvKXnpCM8","executionInfo":{"status":"aborted","timestamp":1661630965005,"user_tz":240,"elapsed":6,"user":{"displayName":"Lobiten AI-ML","userId":"13610020337788558431"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["deeplift = explain.DeepLift(lightning_model)\n","attribution = deeplift.attribute(img, target=tgt)"],"metadata":{"id":"FtLn_oNOpCPE","executionInfo":{"status":"aborted","timestamp":1661630965006,"user_tz":240,"elapsed":7,"user":{"displayName":"Lobiten AI-ML","userId":"13610020337788558431"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Le inplace est mauvais dans la construction de notre modèle (analysons le code) --> le inplace est utilisé pour la fonction d'activation (ReLU)"],"metadata":{"id":"j9G9b_8OCsFc"}},{"cell_type":"code","source":[],"metadata":{"id":"BHaQW8J2m12w","executionInfo":{"status":"aborted","timestamp":1661630965007,"user_tz":240,"elapsed":8,"user":{"displayName":"Lobiten AI-ML","userId":"13610020337788558431"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"NI-kHBCfmim1","executionInfo":{"status":"aborted","timestamp":1661630965007,"user_tz":240,"elapsed":8,"user":{"displayName":"Lobiten AI-ML","userId":"13610020337788558431"}}},"execution_count":null,"outputs":[]}]}